{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee401a20",
   "metadata": {},
   "source": [
    "# Turkish Text Preprocessing and Cleaning for Setur Complaints Analysis\n",
    "\n",
    "This notebook demonstrates comprehensive data preprocessing and cleaning techniques specifically designed for Turkish text analysis. We'll process Setur customer complaints to prepare them for sentiment analysis and topic modeling.\n",
    "\n",
    "## Key Preprocessing Steps:\n",
    "1. **Data Loading and Exploration**\n",
    "2. **Text Normalization** - Lowercasing and Unicode normalization\n",
    "3. **Cleaning** - Remove punctuation, URLs, HTML tags, emojis\n",
    "4. **Turkish Character Preservation** - Handle Turkish-specific characters (Ã§, ÄŸ, Ä±, Ã¶, ÅŸ, Ã¼)\n",
    "5. **Stopword Removal** - Remove Turkish function words\n",
    "6. **Optional Lemmatization** - Reduce words to root forms (important for agglutinative Turkish)\n",
    "\n",
    "## Dataset Overview:\n",
    "- **Source**: Setur tourism company complaints from sikayetvar.com\n",
    "- **Text Fields**: complaint titles, full complaint text, company responses\n",
    "- **Language**: Turkish with potential slang and informal writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7915860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Set\n",
    "\n",
    "# Advanced Turkish NLP libraries\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    print(\"NLTK imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"NLTK not available - install with: pip install nltk\")\n",
    "    nltk = None\n",
    "\n",
    "try:\n",
    "    import zeyrek\n",
    "    print(\"Zeyrek (Turkish morphological analyzer) imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"Zeyrek not available - install with: pip install zeyrek\")\n",
    "    zeyrek = None\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    print(\"spaCy imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"spaCy not available - install with: pip install spacy\")\n",
    "    spacy = None\n",
    "\n",
    "try:\n",
    "    from TurkishStemmer import TurkishStemmer\n",
    "    print(\"TurkishStemmer imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"TurkishStemmer not available - install with: pip install turkish-stemmer\")\n",
    "    TurkishStemmer = None\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set Turkish locale for proper character handling\n",
    "import locale\n",
    "try:\n",
    "    locale.setlocale(locale.LC_ALL, 'tr_TR.UTF-8')\n",
    "except:\n",
    "    print(\"Turkish locale not available, using default\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f295aeb4",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0395022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open('setur_complaints.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for text fields that contain actual content\n",
    "text_fields = ['title', 'full_complaint', 'company_response']\n",
    "for field in text_fields:\n",
    "    if field in df.columns:\n",
    "        non_empty = df[field].notna().sum()\n",
    "        print(f\"\\n{field}: {non_empty} non-empty entries\")\n",
    "        if non_empty > 0:\n",
    "            avg_length = df[field].dropna().str.len().mean()\n",
    "            print(f\"  Average length: {avg_length:.1f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90edbad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample complaints to understand the text content\n",
    "print(\"=== SAMPLE COMPLAINT TITLES ===\")\n",
    "for i, title in enumerate(df['title'].dropna().head(5)):\n",
    "    print(f\"{i+1}. {title}\")\n",
    "\n",
    "print(\"\\n=== SAMPLE FULL COMPLAINTS ===\")\n",
    "for i, complaint in enumerate(df['full_complaint'].dropna().head(3)):\n",
    "    print(f\"{i+1}. {complaint[:200]}...\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n=== SAMPLE COMPANY RESPONSES ===\")\n",
    "for i, response in enumerate(df['company_response'].dropna().head(3)):\n",
    "    print(f\"{i+1}. {response[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9513e113",
   "metadata": {},
   "source": [
    "## 2. Turkish Language Characteristics Analysis\n",
    "\n",
    "Before preprocessing, let's analyze the Turkish language characteristics in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9418a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Turkish character usage\n",
    "def analyze_turkish_chars(text_series):\n",
    "    \"\"\"Analyze Turkish-specific character usage in text series\"\"\"\n",
    "    turkish_chars = {'Ã§', 'ÄŸ', 'Ä±', 'Ã¶', 'ÅŸ', 'Ã¼', 'Ã‡', 'Äž', 'Ä°', 'Ã–', 'Åž', 'Ãœ'}\n",
    "    \n",
    "    all_text = ' '.join(text_series.dropna().astype(str))\n",
    "    char_counts = Counter(all_text)\n",
    "    \n",
    "    turkish_char_counts = {char: char_counts.get(char, 0) for char in turkish_chars}\n",
    "    \n",
    "    return turkish_char_counts, len(all_text)\n",
    "\n",
    "# Analyze character distribution in complaints\n",
    "if 'full_complaint' in df.columns:\n",
    "    turkish_chars, total_chars = analyze_turkish_chars(df['full_complaint'])\n",
    "    \n",
    "    print(\"Turkish Character Distribution in Complaints:\")\n",
    "    for char, count in sorted(turkish_chars.items(), key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / total_chars) * 100\n",
    "        print(f\"  {char}: {count:,} ({percentage:.3f}%)\")\n",
    "\n",
    "# Check for common Turkish patterns\n",
    "print(\"\\n=== Common Turkish Patterns Analysis ===\")\n",
    "sample_text = ' '.join(df['full_complaint'].dropna().head(100).astype(str))\n",
    "\n",
    "# Check for common Turkish suffixes\n",
    "turkish_suffixes = ['ler', 'lar', 'den', 'dan', 'nin', 'nÄ±n', 'nun', 'nÃ¼n', \n",
    "                   'ken', 'iken', 'mÄ±ÅŸ', 'miÅŸ', 'muÅŸ', 'mÃ¼ÅŸ']\n",
    "\n",
    "print(\"Common Turkish suffix patterns found:\")\n",
    "for suffix in turkish_suffixes:\n",
    "    pattern = r'\\w+' + suffix + r'\\b'\n",
    "    matches = len(re.findall(pattern, sample_text, re.IGNORECASE))\n",
    "    if matches > 0:\n",
    "        print(f\"  Words ending with '{suffix}': {matches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7410203a",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing Functions\n",
    "\n",
    "Now let's create comprehensive preprocessing functions specifically designed for Turkish text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d7bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Turkish stopwords\n",
    "TURKISH_STOPWORDS = {\n",
    "    # Articles and determiners\n",
    "    'bir', 'bu', 'ÅŸu', 'o', 'her', 'hiÃ§', 'bazÄ±', 'bÃ¼tÃ¼n', 'tÃ¼m',\n",
    "    \n",
    "    # Conjunctions\n",
    "    've', 'veya', 'ya da', 'ama', 'fakat', 'ancak', 'lakin', 'Ã§Ã¼nkÃ¼', 'eÄŸer',\n",
    "    \n",
    "    # Prepositions\n",
    "    'ile', 'iÃ§in', 'den', 'dan', 'de', 'da', 'te', 'ta', 'ye', 'ya',\n",
    "    'nin', 'nÄ±n', 'nun', 'nÃ¼n', 'in', 'Ä±n', 'un', 'Ã¼n',\n",
    "    \n",
    "    # Pronouns\n",
    "    'ben', 'sen', 'o', 'biz', 'siz', 'onlar', 'beni', 'seni', 'onu',\n",
    "    'bizi', 'sizi', 'onlarÄ±', 'benim', 'senin', 'onun', 'bizim', 'sizin',\n",
    "    \n",
    "    # Question particles\n",
    "    'mi', 'mÄ±', 'mu', 'mÃ¼',\n",
    "    \n",
    "    # Common adverbs\n",
    "    'Ã§ok', 'az', 'daha', 'en', 'Ã§ok', 'pek', 'oldukÃ§a', 'gayet',\n",
    "    'hep', 'hiÃ§', 'her', 'zaman', 'ara', 'sÄ±ra',\n",
    "    \n",
    "    # Common verbs (stems)\n",
    "    'ol', 'olmak', 'et', 'etmek', 'yap', 'yapmak', 'ver', 'vermek',\n",
    "    'al', 'almak', 'gel', 'gelmek', 'git', 'gitmek', 'var', 'yok',\n",
    "    \n",
    "    # Others\n",
    "    'ki', 'gibi', 'kadar', 'diye', 'bile', 'sadece', 'yalnÄ±z',\n",
    "    'hem', 'de', 'da', 'ta', 'te'\n",
    "}\n",
    "\n",
    "print(f\"Turkish stopwords loaded: {len(TURKISH_STOPWORDS)} words\")\n",
    "print(f\"Sample stopwords: {list(TURKISH_STOPWORDS)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b002a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize advanced Turkish NLP libraries\n",
    "def initialize_nlp_libraries():\n",
    "    \"\"\"Initialize and download required NLP resources\"\"\"\n",
    "    \n",
    "    # Initialize NLTK\n",
    "    if nltk:\n",
    "        try:\n",
    "            # Download required NLTK data\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            print(\"âœ“ NLTK resources downloaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not download NLTK data - {e}\")\n",
    "    \n",
    "    # Initialize Zeyrek (Turkish morphological analyzer)\n",
    "    if zeyrek:\n",
    "        try:\n",
    "            global turkish_analyzer\n",
    "            turkish_analyzer = zeyrek.MorphAnalyzer()\n",
    "            print(\"âœ“ Zeyrek Turkish analyzer initialized\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not initialize Zeyrek - {e}\")\n",
    "            turkish_analyzer = None\n",
    "    else:\n",
    "        turkish_analyzer = None\n",
    "    \n",
    "    # Initialize Turkish Stemmer\n",
    "    if TurkishStemmer:\n",
    "        try:\n",
    "            global turkish_stemmer\n",
    "            turkish_stemmer = TurkishStemmer()\n",
    "            print(\"âœ“ Turkish Stemmer initialized\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not initialize Turkish Stemmer - {e}\")\n",
    "            turkish_stemmer = None\n",
    "    else:\n",
    "        turkish_stemmer = None\n",
    "    \n",
    "    # Initialize spaCy Turkish model (if available)\n",
    "    if spacy:\n",
    "        try:\n",
    "            # Try to load Turkish model (needs to be installed separately)\n",
    "            global nlp_turkish\n",
    "            nlp_turkish = spacy.load(\"tr_core_news_sm\")\n",
    "            print(\"âœ“ spaCy Turkish model loaded\")\n",
    "        except OSError:\n",
    "            print(\"Warning: spaCy Turkish model not found\")\n",
    "            print(\"Install with: python -m spacy download tr_core_news_sm\")\n",
    "            nlp_turkish = None\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not initialize spaCy - {e}\")\n",
    "            nlp_turkish = None\n",
    "    else:\n",
    "        nlp_turkish = None\n",
    "    \n",
    "    return {\n",
    "        'nltk_available': nltk is not None,\n",
    "        'zeyrek_available': turkish_analyzer is not None,\n",
    "        'stemmer_available': turkish_stemmer is not None,\n",
    "        'spacy_available': nlp_turkish is not None\n",
    "    }\n",
    "\n",
    "# Initialize libraries\n",
    "print(\"Initializing advanced Turkish NLP libraries...\")\n",
    "library_status = initialize_nlp_libraries()\n",
    "print(f\"\\nLibrary status: {library_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63177920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_turkish_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize Turkish text by handling Unicode variations and character issues.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input Turkish text\n",
    "        \n",
    "    Returns:\n",
    "        str: Normalized text\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Normalize Unicode (important for Turkish characters)\n",
    "    import unicodedata\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    \n",
    "    # Handle common Turkish character variations\n",
    "    replacements = {\n",
    "        'Ä°': 'i',  # Turkish capital Ä° to lowercase i\n",
    "        'I': 'Ä±',  # English I to Turkish Ä±\n",
    "        # Handle potential encoding issues\n",
    "        'ÃƒÂ§': 'Ã§', 'ÃƒÂ¶': 'Ã¶', 'ÃƒÂ¼': 'Ã¼', 'Ã„Â±': 'Ä±', 'Ã„Â°': 'i',\n",
    "        'Ã…': 'ÅŸ', 'Ã„Å¾': 'ÄŸ',\n",
    "    }\n",
    "    \n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    \n",
    "    # Convert to lowercase (Turkish-aware)\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_turkish_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean Turkish text by removing unwanted characters while preserving Turkish letters.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove phone numbers (Turkish format)\n",
    "    text = re.sub(r'\\b0?[5-9][0-9]{2}[\\s-]?[0-9]{3}[\\s-]?[0-9]{2}[\\s-]?[0-9]{2}\\b', '', text)\n",
    "    text = re.sub(r'\\b\\+90[\\s-]?[0-9]{3}[\\s-]?[0-9]{3}[\\s-]?[0-9]{2}[\\s-]?[0-9]{2}\\b', '', text)\n",
    "    \n",
    "    # Remove numbers but keep those that might be part of words\n",
    "    text = re.sub(r'\\b\\d+\\.\\d+\\b', '', text)  # Remove decimal numbers\n",
    "    text = re.sub(r'\\b\\d{4,}\\b', '', text)     # Remove long numbers (IDs, prices, etc.)\n",
    "    \n",
    "    # Remove emojis and special Unicode characters (but keep Turkish chars)\n",
    "    # Keep Turkish characters: a-zA-ZÃ§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄžÄ°Ã–ÅžÃœ\n",
    "    text = re.sub(r'[^\\w\\sÃ§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄžÄ°Ã–ÅžÃœ.,!?;:()\\[\\]\"\\'-]', ' ', text)\n",
    "    \n",
    "    # Remove excessive punctuation\n",
    "    text = re.sub(r'[.,!?;:]{2,}', '.', text)\n",
    "    text = re.sub(r'[\"\\'-]{2,}', '', text)\n",
    "    \n",
    "    # Remove bullet points and list markers\n",
    "    text = re.sub(r'[â€¢â—¦â–ªâ–«â€“â€”]', '', text)\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Test the functions\n",
    "test_text = \"Ã‡ok gÃ¼zel bir Ã¼rÃ¼n! Kesinlikle tavsiye ediyorum. ðŸ‘ www.example.com 0532-123-45-67\"\n",
    "print(\"Original:\", test_text)\n",
    "print(\"Normalized:\", normalize_turkish_text(test_text))\n",
    "print(\"Cleaned:\", clean_turkish_text(normalize_turkish_text(test_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0461517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_keep_meaning(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove punctuation while trying to preserve sentence boundaries and meaning.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        str: Text with punctuation removed\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Replace sentence-ending punctuation with special markers first\n",
    "    text = re.sub(r'[.!?]+\\s+', ' SENTENCE_END ', text)\n",
    "    text = re.sub(r'[.!?]+$', ' SENTENCE_END', text)\n",
    "    \n",
    "    # Remove remaining punctuation (but keep Turkish chars and whitespace)\n",
    "    text = re.sub(r'[^\\w\\sÃ§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄžÄ°Ã–ÅžÃœ]', ' ', text)\n",
    "    \n",
    "    # Clean up whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def tokenize_turkish(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize Turkish text into words.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of tokens\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # Split on whitespace and filter out empty strings\n",
    "    tokens = [token.strip() for token in text.split() if token.strip()]\n",
    "    \n",
    "    # Filter out very short tokens (likely not meaningful)\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(tokens: List[str], custom_stopwords: Set[str] = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Remove Turkish stopwords from token list.\n",
    "    \n",
    "    Args:\n",
    "        tokens (List[str]): List of tokens\n",
    "        custom_stopwords (Set[str]): Additional stopwords to remove\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: Filtered tokens\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    stopwords = TURKISH_STOPWORDS.copy()\n",
    "    if custom_stopwords:\n",
    "        stopwords.update(custom_stopwords)\n",
    "    \n",
    "    # Remove stopwords (case-insensitive)\n",
    "    filtered_tokens = [token for token in tokens \n",
    "                      if token.lower() not in stopwords]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "# Test tokenization and stopword removal\n",
    "test_text = \"Ã§ok gÃ¼zel bir Ã¼rÃ¼n kesinlikle tavsiye ediyorum\"\n",
    "print(\"Test text:\", test_text)\n",
    "tokens = tokenize_turkish(test_text)\n",
    "print(\"Tokens:\", tokens)\n",
    "filtered = remove_stopwords(tokens)\n",
    "print(\"After stopword removal:\", filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19168b88",
   "metadata": {},
   "source": [
    "## 4. Complete Preprocessing Pipeline\n",
    "\n",
    "Now let's create a complete preprocessing pipeline that combines all the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37beee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_turkish_text(text: str, \n",
    "                           remove_stopwords_flag: bool = True,\n",
    "                           min_token_length: int = 2,\n",
    "                           custom_stopwords: Set[str] = None) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for Turkish text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        remove_stopwords_flag (bool): Whether to remove stopwords\n",
    "        min_token_length (int): Minimum token length to keep\n",
    "        custom_stopwords (Set[str]): Additional stopwords\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing original, cleaned text, tokens, and statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    result = {\n",
    "        'original_text': text,\n",
    "        'original_length': len(text) if text else 0,\n",
    "        'cleaned_text': '',\n",
    "        'tokens': [],\n",
    "        'filtered_tokens': [],\n",
    "        'token_count': 0,\n",
    "        'filtered_token_count': 0,\n",
    "        'avg_token_length': 0,\n",
    "        'turkish_char_count': 0\n",
    "    }\n",
    "    \n",
    "    if pd.isna(text) or not isinstance(text, str) or not text.strip():\n",
    "        return result\n",
    "    \n",
    "    # Step 1: Normalize text\n",
    "    normalized = normalize_turkish_text(text)\n",
    "    \n",
    "    # Step 2: Clean text\n",
    "    cleaned = clean_turkish_text(normalized)\n",
    "    \n",
    "    # Step 3: Remove punctuation\n",
    "    no_punct = remove_punctuation_keep_meaning(cleaned)\n",
    "    \n",
    "    result['cleaned_text'] = no_punct\n",
    "    \n",
    "    # Step 4: Tokenize\n",
    "    tokens = tokenize_turkish(no_punct)\n",
    "    \n",
    "    # Filter by minimum length\n",
    "    tokens = [token for token in tokens if len(token) >= min_token_length]\n",
    "    \n",
    "    result['tokens'] = tokens\n",
    "    result['token_count'] = len(tokens)\n",
    "    \n",
    "    # Step 5: Remove stopwords if requested\n",
    "    if remove_stopwords_flag and tokens:\n",
    "        filtered_tokens = remove_stopwords(tokens, custom_stopwords)\n",
    "        result['filtered_tokens'] = filtered_tokens\n",
    "        result['filtered_token_count'] = len(filtered_tokens)\n",
    "    else:\n",
    "        result['filtered_tokens'] = tokens\n",
    "        result['filtered_token_count'] = len(tokens)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    if result['filtered_tokens']:\n",
    "        result['avg_token_length'] = sum(len(token) for token in result['filtered_tokens']) / len(result['filtered_tokens'])\n",
    "    \n",
    "    # Count Turkish characters\n",
    "    turkish_chars = {'Ã§', 'ÄŸ', 'Ä±', 'Ã¶', 'ÅŸ', 'Ã¼'}\n",
    "    result['turkish_char_count'] = sum(1 for char in no_punct.lower() if char in turkish_chars)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test the complete pipeline\n",
    "test_texts = [\n",
    "    \"Ã‡ok gÃ¼zel bir Ã¼rÃ¼n! Kesinlikle tavsiye ediyorum. ðŸ‘\",\n",
    "    \"Setur'dan rezervasyon yapmayÄ±n, yaptÄ±rmayÄ±n. Åžu an yoldayÄ±m, maÄŸdur ettiler.\",\n",
    "    \"60.000 TL Ã¶deyip bu muameleyi gÃ¶rmek kabul edilemez! www.example.com\"\n",
    "]\n",
    "\n",
    "print(\"=== PREPROCESSING PIPELINE TEST ===\")\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"Original: {text}\")\n",
    "    \n",
    "    result = preprocess_turkish_text(text)\n",
    "    print(f\"Cleaned: {result['cleaned_text']}\")\n",
    "    print(f\"Tokens: {result['tokens']}\")\n",
    "    print(f\"Filtered: {result['filtered_tokens']}\")\n",
    "    print(f\"Stats: {result['token_count']} â†’ {result['filtered_token_count']} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b8e3cd",
   "metadata": {},
   "source": [
    "## 5. Apply Preprocessing to Dataset\n",
    "\n",
    "Now let's apply our preprocessing pipeline to the actual Setur complaints dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bf39f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to process the entire dataset\n",
    "def process_complaint_dataset(df: pd.DataFrame, text_columns: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process all text columns in the complaint dataset.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        text_columns (List[str]): List of text column names to process\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with processed text columns\n",
    "    \"\"\"\n",
    "    \n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    for col in text_columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Warning: Column '{col}' not found in dataset\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing column: {col}\")\n",
    "        \n",
    "        # Create new column names\n",
    "        cleaned_col = f\"{col}_cleaned\"\n",
    "        tokens_col = f\"{col}_tokens\"\n",
    "        filtered_col = f\"{col}_filtered\"\n",
    "        \n",
    "        # Initialize new columns\n",
    "        processed_df[cleaned_col] = ''\n",
    "        processed_df[tokens_col] = None\n",
    "        processed_df[filtered_col] = None\n",
    "        \n",
    "        # Process each text entry\n",
    "        non_empty_count = 0\n",
    "        total_original_tokens = 0\n",
    "        total_filtered_tokens = 0\n",
    "        \n",
    "        for idx, text in df[col].items():\n",
    "            if pd.notna(text) and isinstance(text, str) and text.strip():\n",
    "                result = preprocess_turkish_text(text)\n",
    "                \n",
    "                processed_df.at[idx, cleaned_col] = result['cleaned_text']\n",
    "                processed_df.at[idx, tokens_col] = result['tokens']\n",
    "                processed_df.at[idx, filtered_col] = result['filtered_tokens']\n",
    "                \n",
    "                non_empty_count += 1\n",
    "                total_original_tokens += result['token_count']\n",
    "                total_filtered_tokens += result['filtered_token_count']\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"  Processed {non_empty_count} non-empty texts\")\n",
    "        if non_empty_count > 0:\n",
    "            print(f\"  Average tokens per text: {total_original_tokens/non_empty_count:.1f} â†’ {total_filtered_tokens/non_empty_count:.1f}\")\n",
    "            reduction_pct = (1 - total_filtered_tokens/total_original_tokens) * 100 if total_original_tokens > 0 else 0\n",
    "            print(f\"  Token reduction: {reduction_pct:.1f}%\")\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "# Apply preprocessing to our dataset\n",
    "text_columns_to_process = ['title', 'full_complaint', 'company_response']\n",
    "\n",
    "print(\"Starting dataset preprocessing...\")\n",
    "processed_df = process_complaint_dataset(df, text_columns_to_process)\n",
    "\n",
    "print(f\"\\nProcessing complete! Dataset shape: {processed_df.shape}\")\n",
    "print(f\"New columns added: {[col for col in processed_df.columns if col not in df.columns]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a077884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample processed results\n",
    "print(\"=== SAMPLE PROCESSED RESULTS ===\")\n",
    "\n",
    "# Show examples for each text field\n",
    "for col in ['title', 'full_complaint', 'company_response']:\n",
    "    if col in df.columns:\n",
    "        # Find a non-empty example\n",
    "        sample_idx = df[col].dropna().index[0] if not df[col].dropna().empty else None\n",
    "        \n",
    "        if sample_idx is not None:\n",
    "            print(f\"\\n--- {col.upper()} EXAMPLE ---\")\n",
    "            print(f\"Original: {df.loc[sample_idx, col][:150]}...\")\n",
    "            print(f\"Cleaned:  {processed_df.loc[sample_idx, f'{col}_cleaned'][:150]}...\")\n",
    "            print(f\"Tokens:   {processed_df.loc[sample_idx, f'{col}_tokens'][:10]}...\")\n",
    "            print(f\"Filtered: {processed_df.loc[sample_idx, f'{col}_filtered'][:10]}...\")\n",
    "\n",
    "# Create an enhanced function to process the entire dataset with advanced libraries\n",
    "def process_complaint_dataset_advanced(df: pd.DataFrame, text_columns: List[str], \n",
    "                                     processing_method: str = 'auto') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process all text columns in the complaint dataset using advanced Turkish NLP libraries.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        text_columns (List[str]): List of text column names to process\n",
    "        processing_method (str): 'auto', 'basic', 'zeyrek', 'stemmer', 'spacy', or 'all'\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with processed text columns\n",
    "    \"\"\"\n",
    "    \n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Determine the best available method\n",
    "    if processing_method == 'auto':\n",
    "        if turkish_analyzer:\n",
    "            processing_method = 'zeyrek'\n",
    "            print(\"Using Zeyrek lemmatization (best available)\")\n",
    "        elif turkish_stemmer:\n",
    "            processing_method = 'stemmer'\n",
    "            print(\"Using Turkish Stemmer (Zeyrek not available)\")\n",
    "        elif nlp_turkish:\n",
    "            processing_method = 'spacy'\n",
    "            print(\"Using spaCy (Zeyrek and Stemmer not available)\")\n",
    "        else:\n",
    "            processing_method = 'basic'\n",
    "            print(\"Using basic preprocessing (advanced libraries not available)\")\n",
    "    \n",
    "    for col in text_columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Warning: Column '{col}' not found in dataset\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing column: {col} with method: {processing_method}\")\n",
    "        \n",
    "        # Create new column names\n",
    "        cleaned_col = f\"{col}_cleaned\"\n",
    "        tokens_col = f\"{col}_tokens\"\n",
    "        filtered_col = f\"{col}_filtered\"\n",
    "        \n",
    "        # Advanced columns\n",
    "        if processing_method in ['zeyrek', 'all']:\n",
    "            lemmas_col = f\"{col}_lemmas\"\n",
    "            processed_df[lemmas_col] = None\n",
    "        \n",
    "        if processing_method in ['stemmer', 'all']:\n",
    "            stems_col = f\"{col}_stems\"\n",
    "            processed_df[stems_col] = None\n",
    "        \n",
    "        if processing_method in ['spacy', 'all']:\n",
    "            entities_col = f\"{col}_entities\"\n",
    "            pos_col = f\"{col}_pos_tags\"\n",
    "            processed_df[entities_col] = None\n",
    "            processed_df[pos_col] = None\n",
    "        \n",
    "        # Initialize new columns\n",
    "        processed_df[cleaned_col] = ''\n",
    "        processed_df[tokens_col] = None\n",
    "        processed_df[filtered_col] = None\n",
    "        \n",
    "        # Process each text entry\n",
    "        non_empty_count = 0\n",
    "        total_original_tokens = 0\n",
    "        total_filtered_tokens = 0\n",
    "        \n",
    "        for idx, text in df[col].items():\n",
    "            if pd.notna(text) and isinstance(text, str) and text.strip():\n",
    "                \n",
    "                if processing_method == 'basic':\n",
    "                    result = preprocess_turkish_text(text)\n",
    "                    processed_df.at[idx, cleaned_col] = result['cleaned_text']\n",
    "                    processed_df.at[idx, tokens_col] = result['tokens']\n",
    "                    processed_df.at[idx, filtered_col] = result['filtered_tokens']\n",
    "                    total_original_tokens += result['token_count']\n",
    "                    total_filtered_tokens += result['filtered_token_count']\n",
    "                \n",
    "                else:\n",
    "                    # Use advanced processing\n",
    "                    result = advanced_turkish_preprocess(text, processing_method)\n",
    "                    processed_df.at[idx, cleaned_col] = result['cleaned']\n",
    "                    processed_df.at[idx, tokens_col] = result['tokens']\n",
    "                    \n",
    "                    # Choose the best processed tokens based on method\n",
    "                    if processing_method == 'zeyrek' and result['lemmas']:\n",
    "                        best_tokens = remove_stopwords(result['lemmas'])\n",
    "                        processed_df.at[idx, lemmas_col] = result['lemmas']\n",
    "                    elif processing_method == 'stemmer' and result['stems']:\n",
    "                        best_tokens = remove_stopwords(result['stems'])\n",
    "                        processed_df.at[idx, stems_col] = result['stems']\n",
    "                    elif processing_method == 'spacy' and result['spacy_analysis'].get('lemmas'):\n",
    "                        best_tokens = remove_stopwords(result['spacy_analysis']['lemmas'])\n",
    "                        processed_df.at[idx, entities_col] = result['spacy_analysis'].get('entities', [])\n",
    "                        processed_df.at[idx, pos_col] = result['spacy_analysis'].get('pos_tags', [])\n",
    "                    else:\n",
    "                        best_tokens = remove_stopwords(result['tokens'])\n",
    "                    \n",
    "                    processed_df.at[idx, filtered_col] = best_tokens\n",
    "                    total_original_tokens += len(result['tokens'])\n",
    "                    total_filtered_tokens += len(best_tokens)\n",
    "                \n",
    "                non_empty_count += 1\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"  Processed {non_empty_count} non-empty texts\")\n",
    "        if non_empty_count > 0:\n",
    "            print(f\"  Average tokens per text: {total_original_tokens/non_empty_count:.1f} â†’ {total_filtered_tokens/non_empty_count:.1f}\")\n",
    "            reduction_pct = (1 - total_filtered_tokens/total_original_tokens) * 100 if total_original_tokens > 0 else 0\n",
    "            print(f\"  Token reduction: {reduction_pct:.1f}%\")\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "# Apply advanced preprocessing to our dataset\n",
    "text_columns_to_process = ['title', 'full_complaint', 'company_response']\n",
    "\n",
    "print(\"Starting advanced dataset preprocessing...\")\n",
    "print(f\"Available libraries: {library_status}\")\n",
    "\n",
    "# Let user choose processing method or use auto\n",
    "processing_method = 'auto'  # Change this to 'zeyrek', 'stemmer', 'spacy', or 'basic' if desired\n",
    "\n",
    "processed_df = process_complaint_dataset_advanced(df, text_columns_to_process, processing_method)\n",
    "\n",
    "print(f\"\\nAdvanced processing complete! Dataset shape: {processed_df.shape}\")\n",
    "print(f\"New columns added: {[col for col in processed_df.columns if col not in df.columns]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86b2f63",
   "metadata": {},
   "source": [
    "## 6. Text Analysis and Statistics\n",
    "\n",
    "Let's analyze the preprocessed text to understand the characteristics of our cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37495c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive text statistics\n",
    "def calculate_text_statistics(df: pd.DataFrame, text_columns: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate comprehensive statistics for processed text columns.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Processed dataframe\n",
    "        text_columns (List[str]): Original text column names\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Statistics for each column\n",
    "    \"\"\"\n",
    "    \n",
    "    stats = {}\n",
    "    \n",
    "    for col in text_columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        col_stats = {\n",
    "            'original': {\n",
    "                'total_texts': df[col].notna().sum(),\n",
    "                'avg_length': df[col].dropna().str.len().mean() if not df[col].dropna().empty else 0,\n",
    "                'max_length': df[col].dropna().str.len().max() if not df[col].dropna().empty else 0,\n",
    "                'min_length': df[col].dropna().str.len().min() if not df[col].dropna().empty else 0\n",
    "            },\n",
    "            'processed': {}\n",
    "        }\n",
    "        \n",
    "        # Calculate stats for processed versions\n",
    "        filtered_col = f\"{col}_filtered\"\n",
    "        if filtered_col in df.columns:\n",
    "            # Get all filtered tokens\n",
    "            all_filtered_tokens = []\n",
    "            for tokens in df[filtered_col].dropna():\n",
    "                if isinstance(tokens, list):\n",
    "                    all_filtered_tokens.extend(tokens)\n",
    "            \n",
    "            col_stats['processed'] = {\n",
    "                'total_tokens': len(all_filtered_tokens),\n",
    "                'unique_tokens': len(set(all_filtered_tokens)),\n",
    "                'avg_tokens_per_text': len(all_filtered_tokens) / col_stats['original']['total_texts'] if col_stats['original']['total_texts'] > 0 else 0,\n",
    "                'vocabulary_richness': len(set(all_filtered_tokens)) / len(all_filtered_tokens) if all_filtered_tokens else 0\n",
    "            }\n",
    "            \n",
    "            # Most common tokens\n",
    "            token_counts = Counter(all_filtered_tokens)\n",
    "            col_stats['processed']['most_common_tokens'] = token_counts.most_common(20)\n",
    "        \n",
    "        stats[col] = col_stats\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Calculate and display statistics\n",
    "text_stats = calculate_text_statistics(processed_df, text_columns_to_process)\n",
    "\n",
    "print(\"=== TEXT PROCESSING STATISTICS ===\")\n",
    "for col, stats in text_stats.items():\n",
    "    print(f\"\\n{col.upper()}:\")\n",
    "    \n",
    "    orig = stats['original']\n",
    "    print(f\"  Original texts: {orig['total_texts']}\")\n",
    "    print(f\"  Avg length: {orig['avg_length']:.1f} chars (range: {orig['min_length']}-{orig['max_length']})\")\n",
    "    \n",
    "    if 'processed' in stats and stats['processed']:\n",
    "        proc = stats['processed']\n",
    "        print(f\"  Total tokens: {proc['total_tokens']:,}\")\n",
    "        print(f\"  Unique tokens: {proc['unique_tokens']:,}\")\n",
    "        print(f\"  Avg tokens/text: {proc['avg_tokens_per_text']:.1f}\")\n",
    "        print(f\"  Vocabulary richness: {proc['vocabulary_richness']:.3f}\")\n",
    "        \n",
    "        if proc['most_common_tokens']:\n",
    "            print(f\"  Top 10 tokens: {[token for token, count in proc['most_common_tokens'][:10]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb184db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations of the text statistics\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Turkish Text Preprocessing Analysis - Setur Complaints', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Text length distribution (before preprocessing)\n",
    "ax1 = axes[0, 0]\n",
    "lengths = []\n",
    "for col in ['title', 'full_complaint', 'company_response']:\n",
    "    if col in df.columns:\n",
    "        col_lengths = df[col].dropna().str.len()\n",
    "        if not col_lengths.empty:\n",
    "            lengths.extend(col_lengths.tolist())\n",
    "\n",
    "if lengths:\n",
    "    ax1.hist(lengths, bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax1.set_title('Original Text Length Distribution')\n",
    "    ax1.set_xlabel('Characters')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.axvline(np.mean(lengths), color='red', linestyle='--', label=f'Mean: {np.mean(lengths):.0f}')\n",
    "    ax1.legend()\n",
    "\n",
    "# 2. Token count distribution (after preprocessing)\n",
    "ax2 = axes[0, 1]\n",
    "token_counts = []\n",
    "for col in ['title_filtered', 'full_complaint_filtered', 'company_response_filtered']:\n",
    "    if col in processed_df.columns:\n",
    "        for tokens in processed_df[col].dropna():\n",
    "            if isinstance(tokens, list):\n",
    "                token_counts.append(len(tokens))\n",
    "\n",
    "if token_counts:\n",
    "    ax2.hist(token_counts, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "    ax2.set_title('Processed Token Count Distribution')\n",
    "    ax2.set_xlabel('Number of Tokens')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.axvline(np.mean(token_counts), color='red', linestyle='--', label=f'Mean: {np.mean(token_counts):.1f}')\n",
    "    ax2.legend()\n",
    "\n",
    "# 3. Most common words across all complaints\n",
    "ax3 = axes[1, 0]\n",
    "all_tokens = []\n",
    "for col in ['title_filtered', 'full_complaint_filtered', 'company_response_filtered']:\n",
    "    if col in processed_df.columns:\n",
    "        for tokens in processed_df[col].dropna():\n",
    "            if isinstance(tokens, list):\n",
    "                all_tokens.extend(tokens)\n",
    "\n",
    "if all_tokens:\n",
    "    word_freq = Counter(all_tokens)\n",
    "    top_words = word_freq.most_common(15)\n",
    "    words, counts = zip(*top_words)\n",
    "    \n",
    "    ax3.barh(range(len(words)), counts, color='skyblue', edgecolor='black')\n",
    "    ax3.set_yticks(range(len(words)))\n",
    "    ax3.set_yticklabels(words)\n",
    "    ax3.set_title('Top 15 Most Common Words (After Preprocessing)')\n",
    "    ax3.set_xlabel('Frequency')\n",
    "    ax3.invert_yaxis()\n",
    "\n",
    "# 4. Turkish character usage\n",
    "ax4 = axes[1, 1]\n",
    "turkish_chars = {'Ã§': 0, 'ÄŸ': 0, 'Ä±': 0, 'Ã¶': 0, 'ÅŸ': 0, 'Ã¼': 0}\n",
    "all_text = ' '.join(processed_df['full_complaint_cleaned'].dropna().astype(str))\n",
    "\n",
    "for char in turkish_chars:\n",
    "    turkish_chars[char] = all_text.count(char)\n",
    "\n",
    "if any(turkish_chars.values()):\n",
    "    chars, counts = zip(*turkish_chars.items())\n",
    "    ax4.bar(chars, counts, color='orange', edgecolor='black')\n",
    "    ax4.set_title('Turkish Character Frequency (After Cleaning)')\n",
    "    ax4.set_xlabel('Turkish Characters')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\n=== OVERALL PREPROCESSING SUMMARY ===\")\n",
    "print(f\"Total unique words in vocabulary: {len(set(all_tokens)):,}\")\n",
    "print(f\"Total word instances: {len(all_tokens):,}\")\n",
    "print(f\"Average text length (tokens): {np.mean(token_counts):.1f}\")\n",
    "print(f\"Vocabulary richness: {len(set(all_tokens))/len(all_tokens):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ad79c7",
   "metadata": {},
   "source": [
    "## 7. Advanced Turkish Text Processing \n",
    "\n",
    "\n",
    "For more sophisticated analysis, we can add lemmatization and advanced Turkish NLP features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f69133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turkish text normalization for common internet slang and abbreviations\n",
    "TURKISH_SLANG_DICT = {\n",
    "    # Common abbreviations and slang\n",
    "    'gzl': 'gÃ¼zel',\n",
    "    'bÅ¡ÅŸÅŸ': 'brÃ¼ÅŸÃ¼ÅŸ',  # common typo for 'biÅŸi' (something)\n",
    "    'nslsn': 'nasÄ±lsÄ±n',\n",
    "    'tmm': 'tamam',\n",
    "    'sÄŸl': 'saÄŸol',\n",
    "    'tÅŸk': 'teÅŸekkÃ¼r',\n",
    "    'krdsm': 'kardeÅŸim',\n",
    "    'mslm': 'mÃ¼slÃ¼man',\n",
    "    \n",
    "    # Missing diacritics (common in informal text)\n",
    "    'gÃ¼zel': 'gÃ¼zel',  # already correct\n",
    "    'tesekkur': 'teÅŸekkÃ¼r',\n",
    "    'cok': 'Ã§ok',\n",
    "    'guzel': 'gÃ¼zel',\n",
    "    'musteri': 'mÃ¼ÅŸteri',\n",
    "    'hizmet': 'hizmet',\n",
    "    'kalite': 'kalite',\n",
    "    'hosgeldin': 'hoÅŸgeldin',\n",
    "    \n",
    "    # Common misspellings\n",
    "    'deil': 'deÄŸil',\n",
    "    'bisi': 'birÅŸey',\n",
    "    'nasi': 'nasÄ±l',\n",
    "    'olcak': 'olacak',\n",
    "    'gidcek': 'gidecek'\n",
    "}\n",
    "\n",
    "def normalize_slang_and_typos(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize Turkish slang and common typos.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        str: Normalized text\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    words = text.split()\n",
    "    normalized_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Check if word (lowercase) is in slang dictionary\n",
    "        lower_word = word.lower()\n",
    "        if lower_word in TURKISH_SLANG_DICT:\n",
    "            normalized_words.append(TURKISH_SLANG_DICT[lower_word])\n",
    "        else:\n",
    "            normalized_words.append(word)\n",
    "    \n",
    "    return ' '.join(normalized_words)\n",
    "\n",
    "# Simple Turkish stemming (basic suffix removal)\n",
    "def simple_turkish_stem(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Simple Turkish stemming by removing common suffixes.\n",
    "    This is a basic implementation - for production use, consider using \n",
    "    specialized Turkish NLP libraries like Zeyrek or TurkishStemmer.\n",
    "    \n",
    "    Args:\n",
    "        word (str): Input word\n",
    "        \n",
    "    Returns:\n",
    "        str: Stemmed word\n",
    "    \"\"\"\n",
    "    if len(word) < 4:  # Don't stem very short words\n",
    "        return word\n",
    "    \n",
    "    # Common Turkish suffixes (simplified)\n",
    "    suffixes = [\n",
    "        # Plural\n",
    "        'ler', 'lar',\n",
    "        # Possessive\n",
    "        'nin', 'nÄ±n', 'nun', 'nÃ¼n',\n",
    "        # Accusative\n",
    "        'yi', 'yÄ±', 'yu', 'yÃ¼',\n",
    "        # Locative\n",
    "        'de', 'da', 'te', 'ta',\n",
    "        # Ablative\n",
    "        'den', 'dan', 'ten', 'tan',\n",
    "        # Past tense\n",
    "        'mÄ±ÅŸ', 'miÅŸ', 'muÅŸ', 'mÃ¼ÅŸ',\n",
    "        # Progressive\n",
    "        'yor',\n",
    "        # Adjective suffixes\n",
    "        'li', 'lÄ±', 'lu', 'lÃ¼',\n",
    "        'siz', 'sÄ±z', 'suz', 'sÃ¼z'\n",
    "    ]\n",
    "    \n",
    "    word_lower = word.lower()\n",
    "    \n",
    "    # Try to remove suffixes (longest first)\n",
    "    for suffix in sorted(suffixes, key=len, reverse=True):\n",
    "        if word_lower.endswith(suffix) and len(word_lower) > len(suffix) + 2:\n",
    "            return word_lower[:-len(suffix)]\n",
    "    \n",
    "    return word_lower\n",
    "\n",
    "# Test advanced normalization\n",
    "test_texts_advanced = [\n",
    "    \"cok gzl bir urun tsk ederim\",\n",
    "    \"mÃ¼ÅŸteriler memnun kalmÄ±ÅŸ\",\n",
    "    \"otellerde kalitesiz hizmet\"\n",
    "]\n",
    "\n",
    "print(\"=== ADVANCED TURKISH PROCESSING TEST ===\")\n",
    "for text in test_texts_advanced:\n",
    "    print(f\"\\nOriginal: {text}\")\n",
    "    \n",
    "    # Apply slang normalization\n",
    "    normalized = normalize_slang_and_typos(text)\n",
    "    print(f\"Slang normalized: {normalized}\")\n",
    "    \n",
    "    # Apply basic stemming\n",
    "    words = normalized.split()\n",
    "    stemmed_words = [simple_turkish_stem(word) for word in words]\n",
    "    print(f\"Stemmed: {' '.join(stemmed_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b8741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison of preprocessing methods\n",
    "def compare_preprocessing_methods(text: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare different Turkish preprocessing methods side by side.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Comparison results\n",
    "    \"\"\"\n",
    "    \n",
    "    methods_results = []\n",
    "    \n",
    "    # 1. Basic preprocessing (our original method)\n",
    "    basic_result = preprocess_turkish_text(text)\n",
    "    methods_results.append({\n",
    "        'Method': 'Basic (Custom)',\n",
    "        'Tokens': len(basic_result['filtered_tokens']),\n",
    "        'Sample_Words': ' '.join(basic_result['filtered_tokens'][:5]),\n",
    "        'Available': True\n",
    "    })\n",
    "    \n",
    "    # 2. NLTK preprocessing\n",
    "    if nltk:\n",
    "        nltk_tokens = nltk_tokenize_turkish(clean_turkish_text(normalize_turkish_text(text)))\n",
    "        nltk_stops = get_nltk_turkish_stopwords()\n",
    "        if nltk_stops:\n",
    "            nltk_filtered = [t for t in nltk_tokens if t.lower() not in nltk_stops]\n",
    "        else:\n",
    "            nltk_filtered = remove_stopwords(nltk_tokens)\n",
    "        \n",
    "        methods_results.append({\n",
    "            'Method': 'NLTK',\n",
    "            'Tokens': len(nltk_filtered),\n",
    "            'Sample_Words': ' '.join(nltk_filtered[:5]),\n",
    "            'Available': True\n",
    "        })\n",
    "    else:\n",
    "        methods_results.append({\n",
    "            'Method': 'NLTK',\n",
    "            'Tokens': 0,\n",
    "            'Sample_Words': 'Not available',\n",
    "            'Available': False\n",
    "        })\n",
    "    \n",
    "    # 3. Zeyrek lemmatization\n",
    "    if turkish_analyzer:\n",
    "        zeyrek_result = advanced_turkish_preprocess(text, 'zeyrek')\n",
    "        lemma_filtered = remove_stopwords(zeyrek_result['lemmas'])\n",
    "        \n",
    "        methods_results.append({\n",
    "            'Method': 'Zeyrek (Lemmas)',\n",
    "            'Tokens': len(lemma_filtered),\n",
    "            'Sample_Words': ' '.join(lemma_filtered[:5]),\n",
    "            'Available': True\n",
    "        })\n",
    "    else:\n",
    "        methods_results.append({\n",
    "            'Method': 'Zeyrek (Lemmas)',\n",
    "            'Tokens': 0,\n",
    "            'Sample_Words': 'Not available',\n",
    "            'Available': False\n",
    "        })\n",
    "    \n",
    "    # 4. Turkish Stemmer\n",
    "    if turkish_stemmer:\n",
    "        stemmer_result = advanced_turkish_preprocess(text, 'stemmer')\n",
    "        stems_filtered = remove_stopwords(stemmer_result['stems'])\n",
    "        \n",
    "        methods_results.append({\n",
    "            'Method': 'Turkish Stemmer',\n",
    "            'Tokens': len(stems_filtered),\n",
    "            'Sample_Words': ' '.join(stems_filtered[:5]),\n",
    "            'Available': True\n",
    "        })\n",
    "    else:\n",
    "        methods_results.append({\n",
    "            'Method': 'Turkish Stemmer',\n",
    "            'Tokens': 0,\n",
    "            'Sample_Words': 'Not available',\n",
    "            'Available': False\n",
    "        })\n",
    "    \n",
    "    # 5. spaCy processing\n",
    "    if nlp_turkish:\n",
    "        spacy_result = advanced_turkish_preprocess(text, 'spacy')\n",
    "        spacy_lemmas = spacy_result['spacy_analysis'].get('lemmas', [])\n",
    "        spacy_filtered = remove_stopwords(spacy_lemmas)\n",
    "        \n",
    "        methods_results.append({\n",
    "            'Method': 'spaCy',\n",
    "            'Tokens': len(spacy_filtered),\n",
    "            'Sample_Words': ' '.join(spacy_filtered[:5]),\n",
    "            'Available': True\n",
    "        })\n",
    "    else:\n",
    "        methods_results.append({\n",
    "            'Method': 'spaCy',\n",
    "            'Tokens': 0,\n",
    "            'Sample_Words': 'Not available',\n",
    "            'Available': False\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(methods_results)\n",
    "\n",
    "# Test comparison on sample complaints\n",
    "print(\"\\n=== PREPROCESSING METHODS COMPARISON ===\")\n",
    "sample_complaint = \"MÃ¼ÅŸteri hizmetlerinden Ã§ok memnun kalmadÄ±k, otellerdeki hizmet kalitesi beklentilerimizin altÄ±ndaydÄ± ve rezervasyonumuzla ilgili sorunlar yaÅŸadÄ±k.\"\n",
    "\n",
    "print(f\"Sample text: {sample_complaint}\")\n",
    "print(\"\\nComparison of different preprocessing methods:\")\n",
    "comparison_df = compare_preprocessing_methods(sample_complaint)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Performance benchmarking\n",
    "def benchmark_preprocessing_methods(texts: List[str], num_runs: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Benchmark the performance of different preprocessing methods.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): List of texts to process\n",
    "        num_runs (int): Number of runs for averaging\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Benchmark results\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for method_name, method_func in [\n",
    "        ('Basic', lambda t: preprocess_turkish_text(t)),\n",
    "        ('NLTK', lambda t: nltk_tokenize_turkish(clean_turkish_text(normalize_turkish_text(t))) if nltk else []),\n",
    "        ('Zeyrek', lambda t: advanced_turkish_preprocess(t, 'zeyrek') if turkish_analyzer else {}),\n",
    "        ('Stemmer', lambda t: advanced_turkish_preprocess(t, 'stemmer') if turkish_stemmer else {}),\n",
    "        ('spaCy', lambda t: advanced_turkish_preprocess(t, 'spacy') if nlp_turkish else {})\n",
    "    ]:\n",
    "        \n",
    "        times = []\n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            for text in texts:\n",
    "                try:\n",
    "                    method_func(text)\n",
    "                except:\n",
    "                    pass  # Skip errors for unavailable methods\n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        results.append({\n",
    "            'Method': method_name,\n",
    "            'Avg_Time_Seconds': round(avg_time, 4),\n",
    "            'Texts_Per_Second': round(len(texts) / avg_time, 2) if avg_time > 0 else 0\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run benchmark on a sample of complaints\n",
    "if 'df' in locals() and not df.empty:\n",
    "    sample_texts = df['full_complaint'].dropna().head(10).tolist()\n",
    "    if sample_texts:\n",
    "        print(\"\\n=== PERFORMANCE BENCHMARK ===\")\n",
    "        print(f\"Benchmarking on {len(sample_texts)} complaint texts...\")\n",
    "        benchmark_results = benchmark_preprocessing_methods(sample_texts)\n",
    "        print(benchmark_results.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nDataset not loaded yet - benchmark will run after data loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d6a142",
   "metadata": {},
   "source": [
    "## 8. Save Processed Data\n",
    "\n",
    "Finally, let's save our preprocessed data for further analysis.\n",
    "\n",
    "# Advanced Turkish Text Processing with Professional Libraries\n",
    "\n",
    "# 1. NLTK-based tokenization and stopword removal\n",
    "def nltk_tokenize_turkish(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize Turkish text using NLTK.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of tokens\n",
    "    \"\"\"\n",
    "    if not nltk or pd.isna(text) or not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Use NLTK's word tokenizer\n",
    "        tokens = word_tokenize(text, language='turkish')\n",
    "        # Filter out punctuation and short tokens\n",
    "        tokens = [token for token in tokens if token.isalnum() and len(token) > 1]\n",
    "        return tokens\n",
    "    except:\n",
    "        # Fallback to simple tokenization\n",
    "        return text.split()\n",
    "\n",
    "def get_nltk_turkish_stopwords() -> Set[str]:\n",
    "    \"\"\"\n",
    "    Get Turkish stopwords from NLTK if available.\n",
    "    \n",
    "    Returns:\n",
    "        Set[str]: Set of Turkish stopwords\n",
    "    \"\"\"\n",
    "    if not nltk:\n",
    "        return set()\n",
    "    \n",
    "    try:\n",
    "        # Get Turkish stopwords from NLTK\n",
    "        turkish_stops = set(stopwords.words('turkish'))\n",
    "        print(f\"NLTK Turkish stopwords: {len(turkish_stops)} words\")\n",
    "        return turkish_stops\n",
    "    except:\n",
    "        print(\"NLTK Turkish stopwords not available\")\n",
    "        return set()\n",
    "\n",
    "# 2. Zeyrek-based morphological analysis\n",
    "def zeyrek_lemmatize(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Lemmatize Turkish word using Zeyrek morphological analyzer.\n",
    "    \n",
    "    Args:\n",
    "        word (str): Input word\n",
    "        \n",
    "    Returns:\n",
    "        str: Lemmatized word\n",
    "    \"\"\"\n",
    "    if not turkish_analyzer or not word:\n",
    "        return word\n",
    "    \n",
    "    try:\n",
    "        # Analyze the word morphologically\n",
    "        analyses = turkish_analyzer.lemmatize(word)\n",
    "        if analyses:\n",
    "            # Return the first (most likely) lemma\n",
    "            return analyses[0][1]  # [1] is the lemma, [0] is the analysis\n",
    "        return word\n",
    "    except:\n",
    "        return word\n",
    "\n",
    "def zeyrek_analyze_morphology(word: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get detailed morphological analysis using Zeyrek.\n",
    "    \n",
    "    Args:\n",
    "        word (str): Input word\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: Morphological analyses\n",
    "    \"\"\"\n",
    "    if not turkish_analyzer or not word:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        analyses = turkish_analyzer.analyze(word)\n",
    "        return [str(analysis) for analysis in analyses[:3]]  # Top 3 analyses\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# 3. Professional Turkish Stemmer\n",
    "def professional_turkish_stem(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Stem Turkish word using professional Turkish stemmer.\n",
    "    \n",
    "    Args:\n",
    "        word (str): Input word\n",
    "        \n",
    "    Returns:\n",
    "        str: Stemmed word\n",
    "    \"\"\"\n",
    "    if not turkish_stemmer or not word:\n",
    "        return word\n",
    "    \n",
    "    try:\n",
    "        return turkish_stemmer.stem(word)\n",
    "    except:\n",
    "        return word\n",
    "\n",
    "# 4. spaCy-based processing\n",
    "def spacy_process_turkish(text: str) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Process Turkish text using spaCy for NER, POS tagging, etc.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Processing results\n",
    "    \"\"\"\n",
    "    if not nlp_turkish or not text:\n",
    "        return {'tokens': [], 'entities': [], 'pos_tags': []}\n",
    "    \n",
    "    try:\n",
    "        doc = nlp_turkish(text)\n",
    "        \n",
    "        return {\n",
    "            'tokens': [token.text for token in doc],\n",
    "            'lemmas': [token.lemma_ for token in doc],\n",
    "            'pos_tags': [(token.text, token.pos_, token.tag_) for token in doc],\n",
    "            'entities': [(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc.ents],\n",
    "            'noun_phrases': [chunk.text for chunk in doc.noun_chunks]\n",
    "        }\n",
    "    except:\n",
    "        return {'tokens': [], 'entities': [], 'pos_tags': []}\n",
    "\n",
    "# Combined advanced preprocessing function\n",
    "def advanced_turkish_preprocess(text: str, method: str = 'zeyrek') -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Advanced Turkish text preprocessing using professional libraries.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        method (str): Processing method ('zeyrek', 'stemmer', 'spacy', 'all')\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Comprehensive processing results\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return {'original': text, 'processed': '', 'tokens': [], 'method': method}\n",
    "    \n",
    "    # Basic cleaning first\n",
    "    cleaned = clean_turkish_text(normalize_turkish_text(text))\n",
    "    \n",
    "    result = {\n",
    "        'original': text,\n",
    "        'cleaned': cleaned,\n",
    "        'method': method,\n",
    "        'tokens': [],\n",
    "        'lemmas': [],\n",
    "        'stems': [],\n",
    "        'spacy_analysis': {},\n",
    "        'morphology': []\n",
    "    }\n",
    "    \n",
    "    # Tokenize using NLTK if available\n",
    "    if nltk:\n",
    "        tokens = nltk_tokenize_turkish(cleaned)\n",
    "    else:\n",
    "        tokens = tokenize_turkish(cleaned)\n",
    "    \n",
    "    result['tokens'] = tokens\n",
    "    \n",
    "    # Apply different processing methods\n",
    "    if method in ['zeyrek', 'all'] and turkish_analyzer:\n",
    "        # Zeyrek lemmatization\n",
    "        lemmas = [zeyrek_lemmatize(token) for token in tokens]\n",
    "        result['lemmas'] = lemmas\n",
    "        \n",
    "        # Sample morphological analysis for first few words\n",
    "        result['morphology'] = [\n",
    "            (token, zeyrek_analyze_morphology(token)) \n",
    "            for token in tokens[:5]  # Analyze first 5 tokens\n",
    "        ]\n",
    "    \n",
    "    if method in ['stemmer', 'all'] and turkish_stemmer:\n",
    "        # Professional stemming\n",
    "        stems = [professional_turkish_stem(token) for token in tokens]\n",
    "        result['stems'] = stems\n",
    "    \n",
    "    if method in ['spacy', 'all'] and nlp_turkish:\n",
    "        # spaCy analysis\n",
    "        result['spacy_analysis'] = spacy_process_turkish(cleaned)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test the advanced processing\n",
    "test_texts_advanced = [\n",
    "    \"MÃ¼ÅŸteri hizmetleri kalitesiz, otellerde konaklamak memnuniyet vermiyor.\",\n",
    "    \"Rezervasyonumuz iptal edildi, paramÄ±zÄ± geri alamadÄ±k.\",\n",
    "    \"Ã‡ok gÃ¼zel bir tatil geÃ§irdik, kesinlikle tavsiye ederim.\"\n",
    "]\n",
    "\n",
    "print(\"=== ADVANCED TURKISH NLP PROCESSING TEST ===\")\n",
    "for i, text in enumerate(test_texts_advanced, 1):\n",
    "    print(f\"\\n--- Test {i} ---\")\n",
    "    print(f\"Original: {text}\")\n",
    "    \n",
    "    # Test different methods\n",
    "    for method in ['zeyrek', 'stemmer', 'spacy']:\n",
    "        if (method == 'zeyrek' and turkish_analyzer) or \\\n",
    "           (method == 'stemmer' and turkish_stemmer) or \\\n",
    "           (method == 'spacy' and nlp_turkish):\n",
    "            \n",
    "            result = advanced_turkish_preprocess(text, method)\n",
    "            print(f\"\\n{method.upper()} processing:\")\n",
    "            print(f\"  Tokens: {result['tokens'][:8]}...\")  # Show first 8 tokens\n",
    "            \n",
    "            if method == 'zeyrek' and result['lemmas']:\n",
    "                print(f\"  Lemmas: {result['lemmas'][:8]}...\")\n",
    "                if result['morphology']:\n",
    "                    print(f\"  Sample morphology: {result['morphology'][0]}\")\n",
    "            \n",
    "            elif method == 'stemmer' and result['stems']:\n",
    "                print(f\"  Stems: {result['stems'][:8]}...\")\n",
    "            \n",
    "            elif method == 'spacy' and result['spacy_analysis']:\n",
    "                spacy_res = result['spacy_analysis']\n",
    "                if spacy_res['entities']:\n",
    "                    print(f\"  Entities: {spacy_res['entities']}\")\n",
    "                if spacy_res['pos_tags']:\n",
    "                    print(f\"  POS tags: {spacy_res['pos_tags'][:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa6af1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of the preprocessing results\n",
    "def create_preprocessing_summary(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Create a summary of preprocessing results.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Processed dataframe\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Summary statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    summary = {\n",
    "        'dataset_info': {\n",
    "            'total_records': len(df),\n",
    "            'processing_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        },\n",
    "        'text_fields': {}\n",
    "    }\n",
    "    \n",
    "    # Analyze each text field\n",
    "    for col in ['title', 'full_complaint', 'company_response']:\n",
    "        if col in df.columns:\n",
    "            filtered_col = f\"{col}_filtered\"\n",
    "            \n",
    "            # Count non-empty texts\n",
    "            non_empty = df[col].notna().sum()\n",
    "            \n",
    "            # Get all tokens for this field\n",
    "            all_tokens = []\n",
    "            if filtered_col in df.columns:\n",
    "                for tokens in df[filtered_col].dropna():\n",
    "                    if isinstance(tokens, list):\n",
    "                        all_tokens.extend(tokens)\n",
    "            \n",
    "            summary['text_fields'][col] = {\n",
    "                'non_empty_texts': int(non_empty),\n",
    "                'total_tokens': len(all_tokens),\n",
    "                'unique_tokens': len(set(all_tokens)),\n",
    "                'avg_tokens_per_text': len(all_tokens) / non_empty if non_empty > 0 else 0,\n",
    "                'top_10_words': Counter(all_tokens).most_common(10) if all_tokens else []\n",
    "            }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Create preprocessing summary\n",
    "preprocessing_summary = create_preprocessing_summary(processed_df)\n",
    "\n",
    "# Display summary\n",
    "print(\"=== PREPROCESSING SUMMARY ===\")\n",
    "print(f\"Dataset: {preprocessing_summary['dataset_info']['total_records']} records\")\n",
    "print(f\"Processed on: {preprocessing_summary['dataset_info']['processing_date']}\")\n",
    "\n",
    "for field, stats in preprocessing_summary['text_fields'].items():\n",
    "    print(f\"\\n{field.upper()}:\")\n",
    "    print(f\"  Non-empty texts: {stats['non_empty_texts']}\")\n",
    "    print(f\"  Total tokens: {stats['total_tokens']:,}\")\n",
    "    print(f\"  Unique tokens: {stats['unique_tokens']:,}\")\n",
    "    print(f\"  Avg tokens/text: {stats['avg_tokens_per_text']:.1f}\")\n",
    "    if stats['top_10_words']:\n",
    "        top_words = [word for word, count in stats['top_10_words']]\n",
    "        print(f\"  Top words: {top_words}\")\n",
    "\n",
    "# Save processed dataset to CSV\n",
    "output_file = 'setur_complaints_processed.csv'\n",
    "processed_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"\\nProcessed dataset saved to: {output_file}\")\n",
    "\n",
    "# Save preprocessing summary to JSON\n",
    "summary_file = 'preprocessing_summary.json'\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(preprocessing_summary, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Preprocessing summary saved to: {summary_file}\")\n",
    "\n",
    "# Create a clean dataset with only essential columns for analysis\n",
    "analysis_columns = [\n",
    "    'id', 'title', 'time', 'supported', 'rating',\n",
    "    'title_cleaned', 'title_filtered',\n",
    "    'full_complaint_cleaned', 'full_complaint_filtered',\n",
    "    'company_response_cleaned', 'company_response_filtered'\n",
    "]\n",
    "\n",
    "# Keep only existing columns\n",
    "analysis_columns = [col for col in analysis_columns if col in processed_df.columns]\n",
    "analysis_df = processed_df[analysis_columns].copy()\n",
    "\n",
    "# Save clean dataset for analysis\n",
    "analysis_file = 'setur_complaints_for_analysis.csv'\n",
    "analysis_df.to_csv(analysis_file, index=False, encoding='utf-8')\n",
    "print(f\"Clean dataset for analysis saved to: {analysis_file}\")\n",
    "\n",
    "print(\"\\n=== PREPROCESSING COMPLETE ===\")\n",
    "print(\"Files created:\")\n",
    "print(f\"  1. {output_file} - Full processed dataset\")\n",
    "print(f\"  2. {analysis_file} - Clean dataset for analysis\")\n",
    "print(f\"  3. {summary_file} - Preprocessing summary\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  - Use the processed tokens for sentiment analysis\")\n",
    "print(\"  - Apply topic modeling (LDA, NMF) on cleaned text\")\n",
    "print(\"  - Perform keyword extraction and entity recognition\")\n",
    "print(\"  - Analyze complaint patterns and trends\")\n",
    "\n",
    "# Create an enhanced summary of the preprocessing results\n",
    "def create_advanced_preprocessing_summary(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Create a comprehensive summary of advanced preprocessing results.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Processed dataframe\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Summary statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    summary = {\n",
    "        'dataset_info': {\n",
    "            'total_records': len(df),\n",
    "            'processing_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'libraries_used': library_status\n",
    "        },\n",
    "        'text_fields': {},\n",
    "        'advanced_features': {}\n",
    "    }\n",
    "    \n",
    "    # Analyze each text field\n",
    "    for col in ['title', 'full_complaint', 'company_response']:\n",
    "        if col in df.columns:\n",
    "            filtered_col = f\"{col}_filtered\"\n",
    "            lemmas_col = f\"{col}_lemmas\"\n",
    "            stems_col = f\"{col}_stems\"\n",
    "            entities_col = f\"{col}_entities\"\n",
    "            \n",
    "            # Count non-empty texts\n",
    "            non_empty = df[col].notna().sum()\n",
    "            \n",
    "            # Get all tokens for this field\n",
    "            all_tokens = []\n",
    "            all_lemmas = []\n",
    "            all_stems = []\n",
    "            all_entities = []\n",
    "            \n",
    "            if filtered_col in df.columns:\n",
    "                for tokens in df[filtered_col].dropna():\n",
    "                    if isinstance(tokens, list):\n",
    "                        all_tokens.extend(tokens)\n",
    "            \n",
    "            if lemmas_col in df.columns:\n",
    "                for lemmas in df[lemmas_col].dropna():\n",
    "                    if isinstance(lemmas, list):\n",
    "                        all_lemmas.extend(lemmas)\n",
    "            \n",
    "            if stems_col in df.columns:\n",
    "                for stems in df[stems_col].dropna():\n",
    "                    if isinstance(stems, list):\n",
    "                        all_stems.extend(stems)\n",
    "            \n",
    "            if entities_col in df.columns:\n",
    "                for entities in df[entities_col].dropna():\n",
    "                    if isinstance(entities, list):\n",
    "                        all_entities.extend([ent[0] for ent in entities if isinstance(ent, tuple)])  # Extract entity text\n",
    "            \n",
    "            field_summary = {\n",
    "                'non_empty_texts': int(non_empty),\n",
    "                'total_tokens': len(all_tokens),\n",
    "                'unique_tokens': len(set(all_tokens)),\n",
    "                'avg_tokens_per_text': len(all_tokens) / non_empty if non_empty > 0 else 0,\n",
    "                'top_10_words': Counter(all_tokens).most_common(10) if all_tokens else []\n",
    "            }\n",
    "            \n",
    "            # Add advanced features if available\n",
    "            if all_lemmas:\n",
    "                field_summary['total_lemmas'] = len(all_lemmas)\n",
    "                field_summary['unique_lemmas'] = len(set(all_lemmas))\n",
    "                field_summary['top_10_lemmas'] = Counter(all_lemmas).most_common(10)\n",
    "            \n",
    "            if all_stems:\n",
    "                field_summary['total_stems'] = len(all_stems)\n",
    "                field_summary['unique_stems'] = len(set(all_stems))\n",
    "                field_summary['top_10_stems'] = Counter(all_stems).most_common(10)\n",
    "            \n",
    "            if all_entities:\n",
    "                field_summary['total_entities'] = len(all_entities)\n",
    "                field_summary['unique_entities'] = len(set(all_entities))\n",
    "                field_summary['top_entities'] = Counter(all_entities).most_common(5)\n",
    "            \n",
    "            summary['text_fields'][col] = field_summary\n",
    "    \n",
    "    # Overall vocabulary comparison\n",
    "    if summary['text_fields']:\n",
    "        all_field_tokens = []\n",
    "        all_field_lemmas = []\n",
    "        all_field_stems = []\n",
    "        \n",
    "        for field_data in summary['text_fields'].values():\n",
    "            if 'top_10_words' in field_data:\n",
    "                all_field_tokens.extend([word for word, count in field_data['top_10_words']])\n",
    "            if 'top_10_lemmas' in field_data:\n",
    "                all_field_lemmas.extend([word for word, count in field_data['top_10_lemmas']])\n",
    "            if 'top_10_stems' in field_data:\n",
    "                all_field_stems.extend([word for word, count in field_data['top_10_stems']])\n",
    "        \n",
    "        summary['advanced_features'] = {\n",
    "            'vocabulary_reduction': {\n",
    "                'tokens_vs_lemmas': len(set(all_field_tokens)) - len(set(all_field_lemmas)) if all_field_lemmas else 0,\n",
    "                'tokens_vs_stems': len(set(all_field_tokens)) - len(set(all_field_stems)) if all_field_stems else 0\n",
    "            },\n",
    "            'processing_efficiency': {\n",
    "                'lemmatization_available': len(all_field_lemmas) > 0,\n",
    "                'stemming_available': len(all_field_stems) > 0,\n",
    "                'entities_extracted': any('total_entities' in field for field in summary['text_fields'].values())\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Create enhanced preprocessing summary\n",
    "preprocessing_summary = create_advanced_preprocessing_summary(processed_df)\n",
    "\n",
    "# Display enhanced summary\n",
    "print(\"=== ADVANCED PREPROCESSING SUMMARY ===\")\n",
    "print(f\"Dataset: {preprocessing_summary['dataset_info']['total_records']} records\")\n",
    "print(f\"Processed on: {preprocessing_summary['dataset_info']['processing_date']}\")\n",
    "print(f\"Libraries used: {preprocessing_summary['dataset_info']['libraries_used']}\")\n",
    "\n",
    "for field, stats in preprocessing_summary['text_fields'].items():\n",
    "    print(f\"\\n{field.upper()}:\")\n",
    "    print(f\"  Non-empty texts: {stats['non_empty_texts']}\")\n",
    "    print(f\"  Total tokens: {stats['total_tokens']:,}\")\n",
    "    print(f\"  Unique tokens: {stats['unique_tokens']:,}\")\n",
    "    print(f\"  Avg tokens/text: {stats['avg_tokens_per_text']:.1f}\")\n",
    "    \n",
    "    if 'total_lemmas' in stats:\n",
    "        print(f\"  Total lemmas: {stats['total_lemmas']:,}\")\n",
    "        print(f\"  Unique lemmas: {stats['unique_lemmas']:,}\")\n",
    "        reduction = stats['unique_tokens'] - stats['unique_lemmas']\n",
    "        print(f\"  Vocabulary reduction (lemmas): {reduction} words ({reduction/stats['unique_tokens']*100:.1f}%)\")\n",
    "    \n",
    "    if 'total_stems' in stats:\n",
    "        print(f\"  Total stems: {stats['total_stems']:,}\")\n",
    "        print(f\"  Unique stems: {stats['unique_stems']:,}\")\n",
    "        reduction = stats['unique_tokens'] - stats['unique_stems']\n",
    "        print(f\"  Vocabulary reduction (stems): {reduction} words ({reduction/stats['unique_tokens']*100:.1f}%)\")\n",
    "    \n",
    "    if 'total_entities' in stats:\n",
    "        print(f\"  Named entities found: {stats['total_entities']}\")\n",
    "        print(f\"  Unique entities: {stats['unique_entities']}\")\n",
    "        if stats['top_entities']:\n",
    "            print(f\"  Top entities: {[ent for ent, count in stats['top_entities']]}\")\n",
    "    \n",
    "    if stats['top_10_words']:\n",
    "        top_words = [word for word, count in stats['top_10_words'][:5]]\n",
    "        print(f\"  Top 5 words: {top_words}\")\n",
    "\n",
    "# Advanced features summary\n",
    "if 'advanced_features' in preprocessing_summary:\n",
    "    adv_features = preprocessing_summary['advanced_features']\n",
    "    print(f\"\\nADVANCED PROCESSING FEATURES:\")\n",
    "    print(f\"  Vocabulary reduction efficiency: {adv_features['vocabulary_reduction']}\")\n",
    "    print(f\"  Processing capabilities: {adv_features['processing_efficiency']}\")\n",
    "\n",
    "# Save enhanced processed dataset\n",
    "output_file = 'setur_complaints_advanced_processed.csv'\n",
    "processed_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"\\nAdvanced processed dataset saved to: {output_file}\")\n",
    "\n",
    "# Save enhanced preprocessing summary\n",
    "summary_file = 'advanced_preprocessing_summary.json'\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(preprocessing_summary, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Advanced preprocessing summary saved to: {summary_file}\")\n",
    "\n",
    "# Create analysis-ready dataset with best available features\n",
    "analysis_columns = ['id', 'title', 'time', 'supported', 'rating']\n",
    "\n",
    "# Add the best available processed columns\n",
    "for col in ['title', 'full_complaint', 'company_response']:\n",
    "    if col in processed_df.columns:\n",
    "        analysis_columns.extend([\n",
    "            f\"{col}_cleaned\",\n",
    "            f\"{col}_filtered\"\n",
    "        ])\n",
    "        \n",
    "        # Add advanced features if available\n",
    "        if f\"{col}_lemmas\" in processed_df.columns:\n",
    "            analysis_columns.append(f\"{col}_lemmas\")\n",
    "        if f\"{col}_stems\" in processed_df.columns:\n",
    "            analysis_columns.append(f\"{col}_stems\")\n",
    "        if f\"{col}_entities\" in processed_df.columns:\n",
    "            analysis_columns.append(f\"{col}_entities\")\n",
    "        if f\"{col}_pos_tags\" in processed_df.columns:\n",
    "            analysis_columns.append(f\"{col}_pos_tags\")\n",
    "\n",
    "# Keep only existing columns\n",
    "analysis_columns = [col for col in analysis_columns if col in processed_df.columns]\n",
    "analysis_df = processed_df[analysis_columns].copy()\n",
    "\n",
    "# Save enhanced analysis dataset\n",
    "analysis_file = 'setur_complaints_for_advanced_analysis.csv'\n",
    "analysis_df.to_csv(analysis_file, index=False, encoding='utf-8')\n",
    "print(f\"Enhanced analysis dataset saved to: {analysis_file}\")\n",
    "\n",
    "print(\"\\n=== ADVANCED PREPROCESSING COMPLETE ===\")\n",
    "print(\"Files created:\")\n",
    "print(f\"  1. {output_file} - Full advanced processed dataset\")\n",
    "print(f\"  2. {analysis_file} - Enhanced dataset for analysis\")\n",
    "print(f\"  3. {summary_file} - Advanced preprocessing summary\")\n",
    "\n",
    "print(\"\\nAdvanced capabilities now available:\")\n",
    "if library_status['zeyrek_available']:\n",
    "    print(\"  âœ“ Morphological analysis and lemmatization (Zeyrek)\")\n",
    "if library_status['stemmer_available']:\n",
    "    print(\"  âœ“ Professional Turkish stemming\")\n",
    "if library_status['spacy_available']:\n",
    "    print(\"  âœ“ Named Entity Recognition and POS tagging (spaCy)\")\n",
    "if library_status['nltk_available']:\n",
    "    print(\"  âœ“ Advanced tokenization and linguistic features (NLTK)\")\n",
    "\n",
    "print(\"\\nNext steps for advanced analysis:\")\n",
    "print(\"  - Use lemmatized text for improved topic modeling\")\n",
    "print(\"  - Apply sentiment analysis on stemmed/lemmatized tokens\")\n",
    "print(\"  - Extract insights from named entities and POS patterns\")\n",
    "print(\"  - Compare performance across different preprocessing methods\")\n",
    "print(\"  - Implement advanced Turkish-specific ML models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916a173c",
   "metadata": {},
   "source": [
    "## 9. Installation Guide and Troubleshooting\n",
    "\n",
    "To use all the advanced Turkish NLP features, you may need to install additional libraries and language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d724d22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation and Setup Guide for Advanced Turkish NLP\n",
    "\n",
    "def print_installation_guide():\n",
    "    \"\"\"\n",
    "    Print comprehensive installation instructions for all Turkish NLP libraries.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== ADVANCED TURKISH NLP INSTALLATION GUIDE ===\")\n",
    "    print(\"\\n1. BASIC REQUIREMENTS:\")\n",
    "    print(\"   pip install nltk zeyrek spacy turkish-stemmer\")\n",
    "    \n",
    "    print(\"\\n2. NLTK SETUP:\")\n",
    "    print(\"   After installing NLTK, download Turkish language data:\")\n",
    "    print(\"   python -c \\\"import nltk; nltk.download('punkt'); nltk.download('stopwords')\\\"\")\n",
    "    \n",
    "    print(\"\\n3. SPACY TURKISH MODEL:\")\n",
    "    print(\"   Install the Turkish language model for spaCy:\")\n",
    "    print(\"   python -m spacy download tr_core_news_sm\")\n",
    "    print(\"   Note: This requires internet connection and ~15MB download\")\n",
    "    \n",
    "    print(\"\\n4. ZEYREK:\")\n",
    "    print(\"   Zeyrek should work out of the box after installation\")\n",
    "    print(\"   It provides morphological analysis for Turkish\")\n",
    "    \n",
    "    print(\"\\n5. TURKISH STEMMER:\")\n",
    "    print(\"   Professional Turkish stemming algorithm\")\n",
    "    print(\"   Works immediately after pip install\")\n",
    "    \n",
    "    print(\"\\n6. VERIFICATION:\")\n",
    "    print(\"   Run the library status check in this notebook to verify all installations\")\n",
    "    \n",
    "    print(\"\\n7. TROUBLESHOOTING:\")\n",
    "    print(\"   - If spaCy model fails: Check internet connection and try again\")\n",
    "    print(\"   - If Zeyrek fails: Try pip install --upgrade zeyrek\")\n",
    "    print(\"   - If NLTK data fails: Try running in admin/sudo mode\")\n",
    "    print(\"   - For Windows users: Some libraries may require Visual C++ Build Tools\")\n",
    "    \n",
    "    print(\"\\n8. ALTERNATIVE MINIMAL SETUP:\")\n",
    "    print(\"   If you have issues with advanced libraries, the notebook will\")\n",
    "    print(\"   fall back to basic preprocessing which requires no additional setup.\")\n",
    "\n",
    "def check_and_install_requirements():\n",
    "    \"\"\"\n",
    "    Check which libraries are available and provide installation hints.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== LIBRARY STATUS CHECK ===\")\n",
    "    \n",
    "    # Check each library\n",
    "    checks = {\n",
    "        'NLTK': nltk is not None,\n",
    "        'Zeyrek': zeyrek is not None,\n",
    "        'spaCy': spacy is not None,\n",
    "        'Turkish Stemmer': TurkishStemmer is not None\n",
    "    }\n",
    "    \n",
    "    for lib_name, available in checks.items():\n",
    "        status = \"âœ“ Available\" if available else \"âœ— Not installed\"\n",
    "        print(f\"  {lib_name}: {status}\")\n",
    "    \n",
    "    # Additional spaCy model check\n",
    "    if spacy:\n",
    "        try:\n",
    "            import spacy\n",
    "            nlp_test = spacy.load(\"tr_core_news_sm\")\n",
    "            print(\"  spaCy Turkish Model: âœ“ Available\")\n",
    "        except OSError:\n",
    "            print(\"  spaCy Turkish Model: âœ— Not installed (run: python -m spacy download tr_core_news_sm)\")\n",
    "    \n",
    "    # NLTK data check\n",
    "    if nltk:\n",
    "        try:\n",
    "            from nltk.corpus import stopwords\n",
    "            stopwords.words('turkish')\n",
    "            print(\"  NLTK Turkish Data: âœ“ Available\")\n",
    "        except LookupError:\n",
    "            print(\"  NLTK Turkish Data: âœ— Not downloaded (run: nltk.download('stopwords'))\")\n",
    "    \n",
    "    print(\"\\nRECOMMENDATIONS:\")\n",
    "    missing_libs = [lib for lib, available in checks.items() if not available]\n",
    "    \n",
    "    if not missing_libs:\n",
    "        print(\"  ðŸŽ‰ All libraries are available! You can use all advanced features.\")\n",
    "    else:\n",
    "        print(f\"  ðŸ“¦ Missing libraries: {', '.join(missing_libs)}\")\n",
    "        print(\"  ðŸ”§ Run the installation commands above to enable all features\")\n",
    "        print(\"  ðŸ“ The notebook will use basic preprocessing for missing libraries\")\n",
    "\n",
    "# Run the status check\n",
    "check_and_install_requirements()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print_installation_guide()\n",
    "\n",
    "# Create a setup script for easy installation\n",
    "setup_script = '''#!/bin/bash\n",
    "# Setup script for Advanced Turkish NLP\n",
    "# Run this script to install all required libraries and data\n",
    "\n",
    "echo \"Installing Turkish NLP libraries...\"\n",
    "pip install nltk zeyrek spacy turkish-stemmer\n",
    "\n",
    "echo \"Downloading NLTK data...\"\n",
    "python -c \"import nltk; nltk.download('punkt'); nltk.download('stopwords')\"\n",
    "\n",
    "echo \"Installing spaCy Turkish model...\"\n",
    "python -m spacy download tr_core_news_sm\n",
    "\n",
    "echo \"Setup complete! Run the notebook to verify installation.\"\n",
    "'''\n",
    "\n",
    "# Save setup script\n",
    "with open('setup_turkish_nlp.sh', 'w', encoding='utf-8') as f:\n",
    "    f.write(setup_script)\n",
    "\n",
    "print(\"\\nðŸ“ Setup script saved as 'setup_turkish_nlp.sh'\")\n",
    "print(\"   On Unix/Mac: chmod +x setup_turkish_nlp.sh && ./setup_turkish_nlp.sh\")\n",
    "print(\"   On Windows: Run each pip/python command individually in cmd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a564697",
   "metadata": {},
   "source": [
    "## 10. Usage Examples and Best Practices\n",
    "\n",
    "Here are examples of how to use the advanced preprocessing pipeline for different analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df86b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage Examples and Best Practices for Turkish Text Analysis\n",
    "\n",
    "def demonstrate_advanced_usage():\n",
    "    \"\"\"\n",
    "    Demonstrate different use cases for the advanced Turkish preprocessing pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== ADVANCED TURKISH NLP USAGE EXAMPLES ===\")\n",
    "    \n",
    "    # Sample complaints for demonstration\n",
    "    sample_complaints = [\n",
    "        \"MÃ¼ÅŸteri hizmetleri kalitesiz, personel ilgisiz davranÄ±yor.\",\n",
    "        \"Otelde konaklamaktan Ã§ok memnun kaldÄ±k, gÃ¼zel bir deneyimdi.\",\n",
    "        \"Rezervasyon iptal edildi, paramÄ±zÄ± geri alamadÄ±k, Ã§ok sinirli oldum.\"\n",
    "    ]\n",
    "    \n",
    "    for i, complaint in enumerate(sample_complaints, 1):\n",
    "        print(f\"\\n--- Example {i}: {complaint[:50]}... ---\")\n",
    "        \n",
    "        # Show different processing approaches\n",
    "        methods = []\n",
    "        if turkish_analyzer:\n",
    "            methods.append(('Lemmatization', 'zeyrek'))\n",
    "        if turkish_stemmer:\n",
    "            methods.append(('Stemming', 'stemmer'))\n",
    "        if nlp_turkish:\n",
    "            methods.append(('spaCy NLP', 'spacy'))\n",
    "        \n",
    "        if not methods:\n",
    "            methods = [('Basic', 'basic')]\n",
    "        \n",
    "        for method_name, method_code in methods:\n",
    "            result = advanced_turkish_preprocess(complaint, method_code)\n",
    "            \n",
    "            print(f\"\\n{method_name} processing:\")\n",
    "            print(f\"  Cleaned: {result['cleaned'][:60]}...\")\n",
    "            \n",
    "            if method_code == 'zeyrek' and result['lemmas']:\n",
    "                filtered_lemmas = remove_stopwords(result['lemmas'])\n",
    "                print(f\"  Key lemmas: {filtered_lemmas[:5]}\")\n",
    "                if result['morphology']:\n",
    "                    word, analyses = result['morphology'][0]\n",
    "                    print(f\"  Morphology example: '{word}' -> {analyses[:1]}\")\n",
    "            \n",
    "            elif method_code == 'stemmer' and result['stems']:\n",
    "                filtered_stems = remove_stopwords(result['stems'])\n",
    "                print(f\"  Key stems: {filtered_stems[:5]}\")\n",
    "            \n",
    "            elif method_code == 'spacy' and result['spacy_analysis']:\n",
    "                spacy_data = result['spacy_analysis']\n",
    "                if spacy_data['entities']:\n",
    "                    print(f\"  Entities: {spacy_data['entities']}\")\n",
    "                if spacy_data['pos_tags']:\n",
    "                    pos_info = [(word, pos) for word, pos, tag in spacy_data['pos_tags'][:3]]\n",
    "                    print(f\"  POS tags: {pos_info}\")\n",
    "            \n",
    "            else:\n",
    "                filtered_basic = remove_stopwords(result['tokens'])\n",
    "                print(f\"  Key tokens: {filtered_basic[:5]}\")\n",
    "\n",
    "def sentiment_analysis_example():\n",
    "    \"\"\"\n",
    "    Example of how to use processed text for sentiment analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n=== SENTIMENT ANALYSIS EXAMPLE ===\")\n",
    "    \n",
    "    # Simple sentiment keywords for Turkish\n",
    "    positive_words = {'gÃ¼zel', 'memnun', 'harika', 'baÅŸarÄ±lÄ±', 'kaliteli', 'temiz', \n",
    "                     'profesyonel', 'hÄ±zlÄ±', 'etkili', 'tavsiye'}\n",
    "    negative_words = {'kÃ¶tÃ¼', 'berbat', 'kalitesiz', 'yavaÅŸ', 'kirli', 'ilgisiz', \n",
    "                     'sinirli', 'maÄŸdur', 'problem', 'ÅŸikayet', 'iptal'}\n",
    "    \n",
    "    def simple_sentiment_score(tokens):\n",
    "        if not tokens:\n",
    "            return 0\n",
    "        \n",
    "        positive_count = sum(1 for token in tokens if token.lower() in positive_words)\n",
    "        negative_count = sum(1 for token in tokens if token.lower() in negative_words)\n",
    "        \n",
    "        total_words = len(tokens)\n",
    "        sentiment_score = (positive_count - negative_count) / total_words if total_words > 0 else 0\n",
    "        \n",
    "        return sentiment_score\n",
    "    \n",
    "    # Test sentiment analysis on sample complaints\n",
    "    if 'processed_df' in locals() and not processed_df.empty:\n",
    "        print(\"\\nSentiment analysis on first 5 complaints:\")\n",
    "        \n",
    "        for idx in processed_df.index[:5]:\n",
    "            complaint_title = processed_df.loc[idx, 'title']\n",
    "            \n",
    "            # Use the best available processed tokens\n",
    "            if 'full_complaint_lemmas' in processed_df.columns:\n",
    "                tokens = processed_df.loc[idx, 'full_complaint_lemmas']\n",
    "                method = 'lemmas'\n",
    "            elif 'full_complaint_stems' in processed_df.columns:\n",
    "                tokens = processed_df.loc[idx, 'full_complaint_stems']\n",
    "                method = 'stems'\n",
    "            else:\n",
    "                tokens = processed_df.loc[idx, 'full_complaint_filtered']\n",
    "                method = 'tokens'\n",
    "            \n",
    "            if isinstance(tokens, list) and tokens:\n",
    "                filtered_tokens = remove_stopwords(tokens)\n",
    "                sentiment = simple_sentiment_score(filtered_tokens)\n",
    "                \n",
    "                sentiment_label = \"Positive\" if sentiment > 0.05 else \"Negative\" if sentiment < -0.05 else \"Neutral\"\n",
    "                \n",
    "                print(f\"\\n{idx+1}. {complaint_title[:40]}...\")\n",
    "                print(f\"   Method: {method}, Score: {sentiment:.3f}, Label: {sentiment_label}\")\n",
    "                print(f\"   Key words: {filtered_tokens[:5]}\")\n",
    "    else:\n",
    "        print(\"No processed data available yet.\")\n",
    "\n",
    "def topic_modeling_preparation_example():\n",
    "    \"\"\"\n",
    "    Example of how to prepare processed text for topic modeling.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n=== TOPIC MODELING PREPARATION EXAMPLE ===\")\n",
    "    \n",
    "    if 'processed_df' in locals() and not processed_df.empty:\n",
    "        # Collect all processed documents\n",
    "        documents = []\n",
    "        \n",
    "        for idx in processed_df.index[:20]:  # Use first 20 for example\n",
    "            # Combine title and complaint for richer context\n",
    "            title_tokens = processed_df.loc[idx, 'title_filtered'] or []\n",
    "            complaint_tokens = processed_df.loc[idx, 'full_complaint_filtered'] or []\n",
    "            \n",
    "            if isinstance(title_tokens, list) and isinstance(complaint_tokens, list):\n",
    "                combined_tokens = title_tokens + complaint_tokens\n",
    "                # Filter out very short documents\n",
    "                if len(combined_tokens) >= 3:\n",
    "                    documents.append(' '.join(combined_tokens))\n",
    "        \n",
    "        print(f\"Prepared {len(documents)} documents for topic modeling\")\n",
    "        print(f\"Sample document: {documents[0][:100]}...\" if documents else \"No documents available\")\n",
    "        \n",
    "        # Basic word frequency analysis\n",
    "        if documents:\n",
    "            all_words = []\n",
    "            for doc in documents:\n",
    "                all_words.extend(doc.split())\n",
    "            \n",
    "            word_freq = Counter(all_words)\n",
    "            print(f\"\\nTotal vocabulary: {len(set(all_words))} unique words\")\n",
    "            print(f\"Most common words: {word_freq.most_common(10)}\")\n",
    "            \n",
    "            # Vocabulary richness\n",
    "            richness = len(set(all_words)) / len(all_words)\n",
    "            print(f\"Vocabulary richness: {richness:.3f}\")\n",
    "    else:\n",
    "        print(\"No processed data available yet.\")\n",
    "\n",
    "def best_practices_summary():\n",
    "    \"\"\"\n",
    "    Provide best practice recommendations for Turkish text analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n=== BEST PRACTICES FOR TURKISH TEXT ANALYSIS ===\")\n",
    "    \n",
    "    practices = {\n",
    "        \"1. Preprocessing Strategy\": [\n",
    "            \"Use lemmatization (Zeyrek) for semantic analysis and topic modeling\",\n",
    "            \"Use stemming for word frequency analysis and search applications\",\n",
    "            \"Use basic tokenization for quick exploratory analysis\",\n",
    "            \"Always preserve Turkish characters during cleaning\"\n",
    "        ],\n",
    "        \"2. Library Selection\": [\n",
    "            \"Zeyrek: Best for morphological analysis and lemmatization\",\n",
    "            \"Turkish Stemmer: Fast and reliable for stemming\",\n",
    "            \"spaCy: Excellent for named entity recognition and POS tagging\",\n",
    "            \"NLTK: Good for additional linguistic features and tokenization\"\n",
    "        ],\n",
    "        \"3. Performance Considerations\": [\n",
    "            \"Zeyrek: Slower but most accurate for Turkish morphology\",\n",
    "            \"Stemmer: Fastest for large-scale processing\",\n",
    "            \"spaCy: Medium speed, rich features\",\n",
    "            \"Basic preprocessing: Fastest, good for initial exploration\"\n",
    "        ],\n",
    "        \"4. Analysis Applications\": [\n",
    "            \"Sentiment Analysis: Use lemmas or stems + Turkish sentiment lexicon\",\n",
    "            \"Topic Modeling: Lemmatized text works best (reduces sparsity)\",\n",
    "            \"Keyword Extraction: Stems are often sufficient\",\n",
    "            \"Named Entity Recognition: Use spaCy for best results\"\n",
    "        ],\n",
    "        \"5. Quality Assurance\": [\n",
    "            \"Always inspect processed output with sample texts\",\n",
    "            \"Compare vocabulary reduction across methods\",\n",
    "            \"Test on domain-specific text (tourism complaints)\",\n",
    "            \"Validate results with native Turkish speakers when possible\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, tips in practices.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for tip in tips:\n",
    "            print(f\"  â€¢ {tip}\")\n",
    "    \n",
    "    print(\"\\n=== RECOMMENDED WORKFLOW ===\")\n",
    "    workflow = [\n",
    "        \"1. Start with basic preprocessing for quick exploration\",\n",
    "        \"2. Use Zeyrek lemmatization for semantic analysis\",\n",
    "        \"3. Apply spaCy for entity extraction and linguistic features\",\n",
    "        \"4. Compare results across methods to choose the best for your task\",\n",
    "        \"5. Optimize preprocessing pipeline based on downstream performance\"\n",
    "    ]\n",
    "    \n",
    "    for step in workflow:\n",
    "        print(f\"  {step}\")\n",
    "\n",
    "# Run all examples\n",
    "demonstrate_advanced_usage()\n",
    "sentiment_analysis_example()\n",
    "topic_modeling_preparation_example()\n",
    "best_practices_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ ADVANCED TURKISH TEXT PREPROCESSING COMPLETE!\")\n",
    "print(\"\\nYour Turkish complaint analysis pipeline is now ready with:\")\n",
    "print(\"  âœ“ Professional morphological analysis\")\n",
    "print(\"  âœ“ Advanced tokenization and normalization\")\n",
    "print(\"  âœ“ Multiple preprocessing strategies\")\n",
    "print(\"  âœ“ Comprehensive evaluation tools\")\n",
    "print(\"  âœ“ Ready-to-use datasets for ML/NLP tasks\")\n",
    "print(\"\\nHappy analyzing! ðŸ‡¹ðŸ‡·ðŸ“Š\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bf6575",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a comprehensive Turkish text preprocessing pipeline specifically designed for customer complaint analysis. It integrates multiple state-of-the-art Turkish NLP libraries to offer different levels of processing sophistication:\n",
    "\n",
    "### Key Features:\n",
    "- **Multi-library support**: NLTK, Zeyrek, spaCy, Turkish-stemmer\n",
    "- **Flexible processing**: Choose between basic, lemmatization, stemming, or full NLP analysis\n",
    "- **Turkish language optimization**: Handles agglutination, character encoding, and linguistic nuances\n",
    "- **Performance benchmarking**: Compare different methods for your specific use case\n",
    "- **Ready-to-use outputs**: Preprocessed datasets ready for sentiment analysis, topic modeling, etc.\n",
    "\n",
    "### Files Generated:\n",
    "1. `setur_complaints_advanced_processed.csv` - Full processed dataset with all features\n",
    "2. `setur_complaints_for_advanced_analysis.csv` - Clean dataset optimized for ML/NLP\n",
    "3. `advanced_preprocessing_summary.json` - Detailed processing statistics\n",
    "4. `setup_turkish_nlp.sh` - Installation script for all dependencies\n",
    "\n",
    "### Next Steps:\n",
    "- Run sentiment analysis on lemmatized text\n",
    "- Apply topic modeling (LDA/NMF) using processed tokens\n",
    "- Extract business insights from named entities\n",
    "- Build Turkish-specific ML models for complaint classification\n",
    "\n",
    "The pipeline automatically falls back to simpler methods if advanced libraries are not available, ensuring it works in any environment while providing the best possible results when fully configured."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
